{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports\n",
    "First we will import all the newly developed modules from the hupml library:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Hupml imports\n",
    "from hupml import LoadConfig\n",
    "from hupml import MlDataFrame\n",
    "from hupml import PipelineBase\n",
    "\n",
    "# Some imports for this demo\n",
    "import pandas as pd\n",
    "import json\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There is one module that has been developed that we won't be using today. It's a database connection class. So if you ever work for a client with a database, you can easily kickstart your project by using this. It includes several methods to read and write to the database from/to Pandas DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# from hupml.database_connection import DbConnection "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need a couple of other variables and methods for this demo specific:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "root_dir = (os.path.abspath(os.path.dirname('../..')))\n",
    "telco_data = pd.read_csv(f'{root_dir}/data/telco_customer_churn.csv')\n",
    "\n",
    "def print_dict_pretty(d):\n",
    "    print(json.dumps(d, indent=4))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pipeline\n",
    "There are a lot of steps involved in training models that are often overlooked. A lot of time goes into data engineering (fixing data gaps, imporving data quality etc.), feature engineering, parameter tuning and reporting your results. In previous projects, we also noticed we had to manipulate a lot of data. The more data source you have and the more models you make, the more data-manipulation-paths you can walk. To not get drowned in ever increasing amount of if-statements, the PipelineBase class has been setup in the hupml package.\n",
    "\n",
    "The idea is that all the steps involved in a Data Science project are registered with one or multiple pipeline classes. For this, you only need to make a new class and inherit from `PipelineBase` from the hupml package. For everybody who doesn't know a lot about Object Oriented Programming (OOP), I think the examples will be illustrative enough. If not, look for someone who can help you."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Build your own class\n",
    "First we inherit from PipelineBase and setup the absolute minimum for a class:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Can't instantiate abstract class Pipeline with abstract methods handle_nans",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-59d189f54f9a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;31m# Instantiate pipeline object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: Can't instantiate abstract class Pipeline with abstract methods handle_nans"
     ]
    }
   ],
   "source": [
    "class Pipeline(PipelineBase):\n",
    "    pass\n",
    "\n",
    "# Instantiate pipeline object\n",
    "p = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, we get an error. Some methods need to be implemented (these methods are the \"abstract\" methods). Let's do that:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() missing 2 required positional arguments: 'df' and 'method_settings'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-5-f91a7b3a717a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;31m# Instantiate pipeline object\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0mp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mPipeline\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m: __init__() missing 2 required positional arguments: 'df' and 'method_settings'"
     ]
    }
   ],
   "source": [
    "class Pipeline(PipelineBase):       \n",
    "    def handle_nans(self):\n",
    "        print('Method \"handle_nans\" was called')\n",
    "\n",
    "# Instantiate pipeline object\n",
    "p = Pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahhhh more errors! The class actually needs two arguments called `df` and `method_settings`. Lets look at that in step 2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Passing your data and pipeline method settings\n",
    " The argument `df` is our data (DataFrame/MlDataFrame) we pass to the `Pipeline` class. The argument `method_settings` is a list of dictionairies containing the methods + arguments that need to be called, in the order specified. This list looks like the following:  \n",
    "```\n",
    "[\n",
    "    {'method1_name': {'argument1_name': argument1_value}}, # Method 1\n",
    "    {'method2_name': {'argument1_name': argument1_value, 'argument2_name': argument2_value}}, # Method 2\n",
    "    ETC....\n",
    "]\n",
    "```\n",
    "\n",
    "Let's say that when our pipeline runs, we only want to run the `handle_nans` methods. We would then do the following:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'handle_nans': None}, {'handle_nans': None}]\n"
     ]
    }
   ],
   "source": [
    "class Pipeline(PipelineBase):       \n",
    "    def handle_nans(self):\n",
    "        print('Method \"handle_nans\" was called')\n",
    "\n",
    "method_settings = [\n",
    "    {'handle_nans': None}, # The method handle_nans doesn't have any arguments, so we set the method argument to None\n",
    "    {'handle_nans': None} # Let's run handle_nans twice in a row\n",
    "]\n",
    "\n",
    "# Instantiate pipeline object\n",
    "p = Pipeline(df=telco_data, method_settings=method_settings)\n",
    "# List pipeline settings\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No more errors, woopwoop! Let's do something a little bit more:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'handle_nans': {}}, {'print_text_from_argument': {'text': 'This is the text passed to the method'}}, {'print_text_from_argument': {'text': 1}}, {'print_predefined_text': None}, {'n_times_squared': {'value': 2, 'n': 2}}, {'print_text_from_argument': {'text': 'Same method is called again, but later in the pipeline'}}]\n"
     ]
    }
   ],
   "source": [
    "class Pipeline(PipelineBase):\n",
    "    def handle_nans(self):\n",
    "        print('Method \"handle_nans\" was called')\n",
    "\n",
    "    def print_text_from_argument(self, text='asfd'):\n",
    "        print(text)\n",
    "\n",
    "    def print_predefined_text(self):\n",
    "        print('Predefined text')\n",
    "\n",
    "    def n_times_squared(self, value: int, n: int):\n",
    "        result = value\n",
    "        for i in range(0, n):\n",
    "            result = result ** 2\n",
    "        print(f'Squaring the number {value} for {n} times in a row gives = {result}')\n",
    "\n",
    "\n",
    "method_settings = [\n",
    "    {'handle_nans':{}},\n",
    "    {'print_text_from_argument': {'text': 'This is the text passed to the method'}},\n",
    "    {'print_text_from_argument': {'text': 1}},\n",
    "    {'print_predefined_text': None},\n",
    "    {'n_times_squared': {'value': 2, 'n': 2}},\n",
    "    {'print_text_from_argument': {'text': 'Same method is called again, but later in the pipeline'}}\n",
    "]\n",
    "\n",
    "# Instantiate pipeline object\n",
    "p = Pipeline(df=telco_data, method_settings=method_settings)\n",
    "# List pipeline settings\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Execute your pipeline\n",
    "The pipeline class contains the method `run`, this will execute your pipeline as defined. Simply call this method:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Method \"handle_nans\" was called\n",
      "This is the text passed to the method\n",
      "1\n",
      "Predefined text\n",
      "Squaring the number 2 for 2 times in a row gives = 16\n",
      "Same method is called again, but later in the pipeline\n"
     ]
    }
   ],
   "source": [
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Load your settings from a configuration file\n",
    "You can also define your pipeline settings in a `.yaml` file and let the pipeline class load this file. For this demo we made the `pipeline_settings_demo.yaml` file:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pipeline:\n",
      "  - print_text_from_argument: {text: 'This is the text passed to the method'}\n",
      "  - print_text_from_argument: {text: 1}\n",
      "  - print_predefined_text:\n",
      "  - n_times_squared: {value: 2, n: 2}\n",
      "  - print_text_from_argument: {text: 'Same method is called again, but later in the pipeline'}\n"
     ]
    }
   ],
   "source": [
    "with open(f'{root_dir}/configs/pipeline_settings_demo.yaml', 'r') as f:\n",
    "    print(f.read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we load this as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'print_text_from_argument': {'text': 'This is the text passed to the method'}}, {'print_text_from_argument': {'text': 1}}, {'print_predefined_text': None}, {'n_times_squared': {'value': 2, 'n': 2}}, {'print_text_from_argument': {'text': 'Same method is called again, but later in the pipeline'}}]\n"
     ]
    }
   ],
   "source": [
    "class Pipeline(PipelineBase):\n",
    "    def handle_nans(self):\n",
    "        print('Method \"handle_nans\" was called')\n",
    "\n",
    "    def print_text_from_argument(self, text='asfd'):\n",
    "        print(text)\n",
    "\n",
    "    def print_predefined_text(self):\n",
    "        print('Predefined text')\n",
    "\n",
    "    def n_times_squared(self, value: int, n: int):\n",
    "        result = value\n",
    "        for i in range(0, n):\n",
    "            result = result ** 2\n",
    "        print(f'Squaring the number {value} for {n} times in a row gives = {result}')\n",
    "\n",
    "# Instantiate pipeline object\n",
    "p = Pipeline.from_yaml_file(df=telco_data, path=f'{root_dir}/configs/pipeline_settings_demo.yaml')\n",
    "# List pipeline settings\n",
    "print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The configuration file doesn't include `handle_nans` like before, so we _shouldn't_ see this when we run it. Now let's run it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "This is the text passed to the method\n",
      "1\n",
      "Predefined text\n",
      "Squaring the number 2 for 2 times in a row gives = 16\n",
      "Same method is called again, but later in the pipeline\n"
     ]
    }
   ],
   "source": [
    "p.run()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That's it! To use the `Pipeline` and `PipelineBase` class correctly, we would like to give a few more notes:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Considerations and advanced usage\n",
    "If you have _(mostly) the same data manipulations_ for each pipeline, you can probably use just a single class as described above. However, if this class becomes extremly large (code smell) and large portions of the code are evident to be only applicable to certain types of pipelines, you might consider multiple inheritance. \n",
    "\n",
    "For example, handling nans is something you probably always want to do, but you might have completely different methods for classification models and regression models. So you might build a `Pipeline` class as above, but make two _extra_ classes `PipelineClassification` and `PipelineRegression` that _inherit_ from your `Pipeline` class. Another example is that you maybe have timeseries and non-timeseries data. Here, too, you might consider using multiple inheritance if that seems logical.\n",
    "\n",
    "We advice you just to start with what we described above, and only refactor when it's deemed necessary."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
